{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 - Small Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "from conllu import *\n",
    "from treebanks import languages, train_corpus, test_corpus, conllu_corpus\n",
    "from collections import defaultdict, Counter\n",
    "from math import log, exp\n",
    "from sys import float_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMMs\n",
    "Write your own code to estimate the transition probabilities and the emission prob- abilities of an HMM, on the basis of (tagged) sentences from a training corpus from Universal Dependencies. Do not forget to involve the start-of-sentence marker ⟨s⟩ and the end-of-sentence marker ⟨/s⟩ in the estimation.\n",
    "The code in this part is concerned with:\n",
    "- counting occurrences of one part of speech following another in a training corpus, \n",
    "- counting occurrences of words together with parts of speech in a training corpus, \n",
    "- relative frequency estimation with smoothing.\n",
    "\n",
    "As discussed in the lectures, smoothing is necessary to avoid zero probabilities for events that were not witnessed in the training corpus. For emission probabilities, implement Witten-Bell smoothing for unigrams, exactly as on slide 23 of Lecture 5 (use 100000 as value of z). This means that we have one smoothed probability distribution over words for each tag. For transition probabilities, implement Witten-Bell smoothing for bigrams of tags, exactly as on slide 22 of that same lecture (and this in turn will make use of Witten-Bell unigram smoothing that you will have implemented before). To be clear: do not use any existing implementations of Witten-Bell, but make your own, based on your understanding of the lecture notes.\n",
    "\n",
    "Further write your own code to implement the Viterbi algorithm, which determines the sequence of tags for a given sentence that has the highest probability. To avoid underflow for long sentences, we need to use log probabilities. Implement code to compute accuracy of POS tagging (the percentage of correctly predicted tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HidddenMarkovModel:\n",
    "\n",
    "    def __init__(self, trainingCorpus, z = 100000): \n",
    "        self.z = z\n",
    "        self.n = 0 # length of training data\n",
    "        self.m = 0 # number of distinct words                   # TODO can be omitted its just len(self.unigramCounts)\n",
    "        self.unigramCounts = defaultdict(int)\n",
    "        self.bigramCount = defaultdict(int)\n",
    "        self.wordTagCounts = defaultdict(int) # dictionary of dictionaries tracking word occurrences per tag\n",
    "        self.tagOccurr = defaultdict(int) # total number of word occurrences for tag t\n",
    "        self.uniqueWordsPerTag = defaultdict(set)\n",
    "        self.uniqueTags = []\n",
    "        self.tagCountOccurrences = Counter() # TODO this iss new version of tagOccurr wout eos and sos markers\n",
    "        #self.tagOccurrences = Counter()\n",
    "        self.__initialiseVars(trainingCorpus)\n",
    "        # print(\"number of distinct words including sentence markers: \", self.m)\n",
    "        # print(\"total number of tokens including sentence markers: \", self.n)\n",
    "\n",
    "\n",
    "\n",
    "    # Initialise all of the vars and train the model\n",
    "    def __initialiseVars(self, trainingCorpus): \n",
    "        # train_sents = conllu_corpus(train_corpus(trainingCorpus))\n",
    "        self.tagCountOccurrences = Counter([token['upos'] for sent in trainingCorpus for token in sent])\n",
    "        # Bigram counts\n",
    "        # for sent in train_sents:                                                # TODO this might overflow if sentence len = 1\n",
    "        for sent in trainingCorpus:\n",
    "            for i in range(len(sent)):\n",
    "                if i == 0:\n",
    "                    self.__addUnigramCount({\"upos\": \"<s>\", \"form\": \"<s>\"})\n",
    "                    self.bigramCount[(\"<s>\", sent[i]['upos'])] += 1\n",
    "                    self.bigramCount[(sent[i]['upos'], sent[i+1]['upos'])] += 1\n",
    "                elif i == len(sent) - 1: \n",
    "                    self.__addUnigramCount({\"upos\": \"</s>\", \"form\": \"</s>\"})\n",
    "                    self.bigramCount[(sent[i]['upos'], \"</s>\")] += 1\n",
    "                else: \n",
    "                    self.bigramCount[(sent[i]['upos'], sent[i+1]['upos'])] += 1\n",
    "                self.__addUnigramCount(sent[i])\n",
    "        self.m = len(self.unigramCounts)\n",
    "\n",
    "        # self.uniqueTags = list(set([token['upos'] for sent in trainingCorpus for token in sent]))\n",
    "        # wordTagPairs = [(token['form'], token['upos']) for sent in trainingCorpus for token in sent]\n",
    "        self.uniqueTags = list(set([token['upos'] for sent in trainingCorpus for token in sent]))\n",
    "        wordTagPairs = [(token['form'], token['upos']) for sent in trainingCorpus for token in sent]\n",
    "        wordTagCounter = Counter(wordTagPairs)\n",
    "        #self.tagOccurrences = Counter(tag for _, tag in wordTagPairs)\n",
    "        self.wordTagCounts = defaultdict(lambda: defaultdict(int))\n",
    "        for (word, tag), count in wordTagCounter.items():\n",
    "            self.wordTagCounts[tag][word] = count\n",
    "\n",
    "        for word, tag in wordTagPairs:\n",
    "            self.uniqueWordsPerTag[tag].add(word)\n",
    "        # T_tag = {tag: len(words) for tag, words in tag_unique_words.items()} # number of possible words per tag\n",
    "            \n",
    "\n",
    "        print(self.tagOccurr)\n",
    "\n",
    "\n",
    "    def __addUnigramCount(self, token): \n",
    "        # tokens consist of a word and a tag\n",
    "        #print(token[\"form\"])\n",
    "        self.n += 1\n",
    "        self.tagOccurr[token['upos']] += 1\n",
    "        self.unigramCounts[token['form']] += 1\n",
    "\n",
    "\n",
    "    # Calculates P(tag) using Witten-Bell smoothing\n",
    "    def __unigramProbability(self, tag):\n",
    "       # print(len(self.tagOccurr))\n",
    "        m = len(self.tagOccurr)\n",
    "        if self.tagOccurr[tag] != 0:\n",
    "            # return self.tagOccurr[tag] / (self.n + m)\n",
    "            return self.tagCountOccurrences[tag] / (self.n + m)\n",
    "        else: \n",
    "            return m / (self.z * (self.n + m))\n",
    "\n",
    "\n",
    "    # Calculates P(word|tag) using Witten-Bell smoothing\n",
    "    def unigramEmissionProbability(self, word, tag):\n",
    "        tag_cts = self.wordTagCounts[tag]\n",
    "        t = len(self.uniqueWordsPerTag[tag])\n",
    "        #n = self.tagOccurr[tag] #\n",
    "        n = self.tagCountOccurrences[tag] #\n",
    "        # print(t)\n",
    "        # print(n)\n",
    "        # print(tag_cts[word])                                      #TODO this is debugging\n",
    "\n",
    "        # If word has been seen with the tag\n",
    "        if word in tag_cts:\n",
    "            return float(tag_cts[word] / (n+t))\n",
    "        else: \n",
    "            return float(t/(self.z * (n + t)))\n",
    "        \n",
    "    # def __wordUnigramProbability(self, word):\n",
    "    #     if word in self.unigramCounts:\n",
    "    #         return self.unigramCounts[word] / (self.n + self.m)\n",
    "    #     else: \n",
    "    #         return self.m / (self.z * (self.m + self.n))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # Calculates P(ti | ti-1) or P(tag|precedingTag) using Witten-Bell smoothing\n",
    "    def bigramTransitionProbability(self, precedingTag, tag):  \n",
    "        precCount = self.tagOccurr[precedingTag]\n",
    "        if self.tagOccurr[precedingTag] == 0:\n",
    "            # if precedingTag count = 0\n",
    "            lambd = 0\n",
    "        else :\n",
    "            # possibleFollowers counts number of tags that were seen following 'precedingTag'\n",
    "            possibleFollowers = sum(1 for (tag1, tag2) in self.bigramCount if tag1 == precedingTag) \n",
    "            lambd = precCount / (precCount + possibleFollowers)\n",
    "\n",
    "        # P(tag | prevtag)\n",
    "        if precCount == 0:\n",
    "            prob_tag_given_preceding = 0\n",
    "        else: \n",
    "            prob_tag_given_preceding = self.bigramCount[(precedingTag, tag)] / precCount\n",
    "        # find the witten bell smoothed probability of tag\n",
    "        unig_prob = self.__unigramProbability(tag)\n",
    "        return (lambd * prob_tag_given_preceding) + ((1 - lambd) * unig_prob)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Adding a list of probabilities represented as log probabilities.\n",
    "    def __logsumexp(self, vals):\n",
    "        min_log_prob = -float_info.max\n",
    "        if len(vals) == 0:\n",
    "            return min_log_prob\n",
    "        m = max(vals)\n",
    "        if m == min_log_prob:\n",
    "            return min_log_prob\n",
    "        else:\n",
    "            return m + log(sum([exp(val - m) for val in vals]))\n",
    "\n",
    "\n",
    "    # def viterbi(self, sent): \n",
    "    #     # Initialize probability table and backpointer table\n",
    "    #     probTable = [[-float_info.max] * len(sent) for _ in range(len(self.uniqueTags))]\n",
    "    #     backpointer = [[-1] * len(sent) for _ in range(len(self.uniqueTags))]\n",
    "        \n",
    "    #     # Initialization step\n",
    "    #     for q in range(len(self.uniqueTags)):\n",
    "    #         transitionProb = log(self.bigramTransitionProbability(\"<s>\", self.uniqueTags[q]))\n",
    "    #         emissionProb = log(self.unigramEmissionProbability(sent[0], self.uniqueTags[q]))\n",
    "    #         probTable[q][0] = transitionProb + emissionProb\n",
    "            \n",
    "    #     # Recursion step\n",
    "    #     for t in range(1, len(sent)):\n",
    "    #         for q in range(len(self.uniqueTags)):\n",
    "    #             maxProb = -float_info.max\n",
    "    #             maxBackpointer = -1\n",
    "                \n",
    "    #             for prev_q in range(len(self.uniqueTags)):\n",
    "    #                 prob = probTable[prev_q][t-1] + \\\n",
    "    #                        log(self.bigramTransitionProbability(self.uniqueTags[prev_q], self.uniqueTags[q])) + \\\n",
    "    #                        log(self.unigramEmissionProbability(sent[t], self.uniqueTags[q]))\n",
    "                    \n",
    "    #                 if prob > maxProb:\n",
    "    #                     maxProb = prob\n",
    "    #                     maxBackpointer = prev_q\n",
    "                \n",
    "    #             probTable[q][t] = maxProb\n",
    "    #             backpointer[q][t] = maxBackpointer\n",
    "                \n",
    "    #     # Termination step\n",
    "    #     maxProb = -float_info.max\n",
    "    #     maxBackpointer = -1\n",
    "        \n",
    "    #     for q in range(len(self.uniqueTags)):\n",
    "    #         prob = probTable[q][len(sent)-1] + \\\n",
    "    #                log(self.bigramTransitionProbability(self.uniqueTags[q], \"</s>\"))\n",
    "            \n",
    "    #         if prob > maxProb:\n",
    "    #             maxProb = prob\n",
    "    #             maxBackpointer = q\n",
    "                \n",
    "    #     # Backtrace to find best path\n",
    "    #     bestPath = []\n",
    "    #     current = maxBackpointer\n",
    "        \n",
    "    #     for t in range(len(sent)-1, -1, -1):\n",
    "    #         bestPath.insert(0, self.uniqueTags[current])\n",
    "    #         current = backpointer[current][t]\n",
    "            \n",
    "    #     return bestPath\n",
    "\n",
    "\n",
    "    def viterbi(self, sent): \n",
    "        # sent should be a \n",
    "        # Use log probablities -> logsumexptrick.py has stuff on how to use them \n",
    "        # # columns = number of words in sent\n",
    "        # num of rows = number of possible tags\n",
    "        # sent.insert(0, \"<s>\")\n",
    "        # sent.append(\"</s>\")\n",
    "        probTable = [[0] * (len(sent) + 1) for _ in range(len(self.uniqueTags))]\n",
    "        maxProb = [-float_info.max] * (len(sent) + 1)\n",
    "        tagPath = [''] * (len(sent) + 1)            \n",
    "\n",
    "        # Initialise \n",
    "        for q in range(len(probTable)):\n",
    "            transitionProb = log(self.bigramTransitionProbability(\"<s>\" , self.uniqueTags[q]))\n",
    "            emissionProb = log(self.unigramEmissionProbability(sent[0], self.uniqueTags[q]))\n",
    "            holder = [transitionProb, emissionProb ]\n",
    "            # print(holder.shape())\n",
    "            # print(type(holder))\n",
    "            probTable[q][0] = self.__logsumexp( holder )\n",
    "            if probTable[q][0] > maxProb[0] :            # TODO esto igual se puede optimizar con un max o algo o pivotando la tabla y en los loops de abajo tmb\n",
    "                maxProb[0] = probTable[q][0]\n",
    "                tagPath[0] = self.uniqueTags[q]\n",
    "\n",
    "                # index_min = min(range(len(values)), key=values.__getitem__)\n",
    "\n",
    "        # Compute\n",
    "        for i in range(1, len(sent)):\n",
    "            for q in range(len(probTable)):\n",
    "                transitionProb = log(self.bigramTransitionProbability(tagPath[i-1] , self.uniqueTags[q]))\n",
    "                emissionProb = log(self.unigramEmissionProbability(sent[i], self.uniqueTags[q]))\n",
    "                prob = self.__logsumexp([maxProb[i - 1], transitionProb, emissionProb ])\n",
    "                probTable[q][i] = prob\n",
    "                if probTable[q][i] > maxProb[i] :            \n",
    "                    maxProb[i] = probTable[q][i]\n",
    "                    tagPath[i] = self.uniqueTags[q]\n",
    "\n",
    "        # Finalise\n",
    "        last_i = len(sent)\n",
    "        for q in range(len(probTable)):\n",
    "            transitionProb = log(self.bigramTransitionProbability(tagPath[last_i - 1] , \"</s>\" ))\n",
    "            probTable[q][last_i] = self.__logsumexp([maxProb[last_i - 1], transitionProb])\n",
    "            if probTable[q][last_i] > maxProb[last_i] :            \n",
    "                maxProb[last_i] = probTable[q][last_i]\n",
    "                tagPath[last_i] = \"ONETOOMANY\"\n",
    "\n",
    "        # Reconstruct the sec\n",
    "        #print(maxProb)\n",
    "        #return tagPath\n",
    "        return tagPath[:-1]\n",
    "    \n",
    "\n",
    "    # Compute the percentage of correctly predicted tags from a given corpus\n",
    "    def posTaggingAccuracy(self, testCorpus):\n",
    "        # testCorpus is a set of tagged sentences from conllu\n",
    "        tagCount = 0\n",
    "        correctTags = 0\n",
    "        for sent in testCorpus:\n",
    "            words = [token[\"form\"] for token in sent]\n",
    "            pred_tags = self.viterbi(words)\n",
    "            given_tags = [token[\"upos\"] for token in sent]\n",
    "            # print(given_tags)\n",
    "            # print(pred_tags)\n",
    "            # print(\"----\")\n",
    "            correctCount = sum(1 for pred, given in zip(pred_tags, given_tags) if pred == given)\n",
    "            tagCount += len(given_tags)\n",
    "            correctTags += correctCount\n",
    "        \n",
    "        if tagCount != 0:\n",
    "            return correctTags / tagCount\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling using HMMs\n",
    "As discussed in the lectures, HMMs can be used to determine the probability of an input sentence, using the forward algorithm. For this, you need to be able to add prob- abilities together that are represented as log probabilities, without getting underflow in the conversion from log probabilities to probabilities and back. See the included logsumexptrick.py for a demonstration.\n",
    "Write your own code to compute the perplexity of a test corpus. (Consider the length of a corpus to be the total number of actual tokens plus the number of sentences; in effect we count one additional end-of-sentence token for each sentence in addition to the actual words and punctuation tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the perplexity of a test corpus given a HMM\n",
    "# use length count as described above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modelling using bigrams\n",
    "Further write your own code to implement estimation of bigram probabilities of input tokens (so words and punctuation tokens, not POS tags). To avoid zero probabilities, you again need Witten-Bell smoothing; you should here be able to reuse the code you implemented earlier for transition probabilities of HMMs.\n",
    "Again, implement your own code to compute perplexity of a test corpus, given a trained bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate bigram probabilities with witten bell smoothing \n",
    "# re-use code for transition probabilities of HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity of a test corpus given a trained bigram model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Run the developed code for the three treebanks. Train using the training parts of the treebanks, and test using the testing parts of the treebanks. (It is good practice to mainly use the development parts during development of the code.) \n",
    "\n",
    "Testing here means:\n",
    "• computing accuracy of POS tagging using an HMM, \n",
    "• computing perplexity using an HMM, and\n",
    "• computing perplexity using a bigram model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN', 'PROPN', 'AUX', 'INTJ', 'ADJ', 'VERB', 'PART', 'ADP', 'DET', 'PRON', 'ADV', 'NUM', 'CCONJ'}\n",
      "defaultdict(<class 'int'>, {'<s>': 4274, 'PRON': 3022, 'AUX': 1732, 'DET': 3805, 'NOUN': 8621, 'ADP': 10791, 'PROPN': 11657, 'VERB': 4595, 'NUM': 933, 'ADJ': 1632, 'CCONJ': 751, '</s>': 4274, 'ADV': 431, 'PART': 366, 'INTJ': 319})\n",
      "Counter({'PROPN': 11657, 'ADP': 10791, 'NOUN': 8621, 'VERB': 4595, 'DET': 3805, 'PRON': 3022, 'AUX': 1732, 'ADJ': 1632, 'NUM': 933, 'CCONJ': 751, 'ADV': 431, 'PART': 366, 'INTJ': 319})\n",
      "11657\n"
     ]
    }
   ],
   "source": [
    "lang = \"en\" \n",
    "#lang = \"orv\" \n",
    "# lang = \"tr\" \n",
    "\n",
    "train_sents = conllu_corpus(train_corpus(lang))\n",
    "test_sents = conllu_corpus(test_corpus(lang))\n",
    "\n",
    "# for sent in train_sents:\n",
    "#     for token in sent:\n",
    "#         print(token['form'], '->', token['upos'], sep='', end=' ')\n",
    "#         #print(token['form'], sep = '', end= \"\")\n",
    "#     print()\n",
    "    \n",
    "uniqueTags = set([token['upos'] for sent in train_sents for token in sent])\n",
    "tagCountOccurrences = Counter([token['upos'] for sent in train_sents for token in sent])\n",
    "print(uniqueTags)\n",
    "hmm = HidddenMarkovModel(trainingCorpus=train_sents)\n",
    "print(tagCountOccurrences)\n",
    "print(tagCountOccurrences['PROPN'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'<s>': 4274, 'PRON': 3022, 'AUX': 1732, 'DET': 3805, 'NOUN': 8621, 'ADP': 10791, 'PROPN': 11657, 'VERB': 4595, 'NUM': 933, 'ADJ': 1632, 'CCONJ': 751, '</s>': 4274, 'ADV': 431, 'PART': 366, 'INTJ': 319})\n",
      "unig:  0.00640863479214099\n",
      "unig:  0.17722855935783693\n",
      "unig:  0.33070088845014806\n",
      "unig:  5.5939453767686737e-08\n",
      "unig:  2.9362061681453316e-07\n",
      "big:  0.12967037225167752\n",
      "big:  0.1788198461119748\n",
      "big:  0.15066673657351579\n",
      "['PRON', 'AUX', 'DET', 'NOUN', 'ADP']\n",
      "['VERB', 'ADP']\n",
      "0.40219915733223716\n",
      "0.4034954407294833\n"
     ]
    }
   ],
   "source": [
    "# HMM testing\n",
    "train_sents = conllu_corpus(train_corpus(\"en\"))\n",
    "test_sents = conllu_corpus(test_corpus(\"en\"))\n",
    "hmm = HidddenMarkovModel(trainingCorpus=train_sents)\n",
    "#hmm = HidddenMarkovModel(\"tr\")\n",
    "\n",
    "print(\"unig: \", hmm.unigramEmissionProbability(\"memphis\", \"PROPN\")) \n",
    "print(\"unig: \", hmm.unigramEmissionProbability(\"show\", \"VERB\")) \n",
    "print(\"unig: \", hmm.unigramEmissionProbability(\"me\", \"PRON\")) \n",
    "print(\"unig: \", hmm.unigramEmissionProbability(\"petersburg\", \"PRON\")) \n",
    "print(\"unig: \", hmm.unigramEmissionProbability(\"me\", \"VERB\")) \n",
    "print(\"big: \", hmm.bigramTransitionProbability(\"ADP\", \"NOUN\")) \n",
    "print(\"big: \", hmm.bigramTransitionProbability(\"NOUN\", \"NOUN\")) \n",
    "print(\"big: \", hmm.bigramTransitionProbability(\"fgfsgs\", \"NOUN\")) \n",
    "\n",
    "# what->PRON is->AUX the->DET cost->NOUN of->ADP\n",
    "print(hmm.viterbi([\"what\", \"is\", \"the\", \"cost\", \"of\"]))\n",
    "print(hmm.viterbi([\"hello\", \"world\"]))\n",
    "\n",
    "print(hmm.posTaggingAccuracy(train_sents))\n",
    "print(hmm.posTaggingAccuracy(test_sents))\n",
    "#what->DET flights->NOUN are->VERB there->PRON from->ADP phoenix->PROPN to->ADP milwaukee->PROPN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - use only testing parts of treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - use only testing part of treebank\n",
    "\n",
    "\n",
    "# call the three functions implemented above \n",
    "# • computing accuracy of POS tagging using an HMM, \n",
    "# • computing perplexity using an HMM, and\n",
    "# • computing perplexity using a bigram model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
